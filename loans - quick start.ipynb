{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f0b7860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64ba8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive # We mount a Google Drive folder where the dataset is.\n",
    "#drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "#path = \"/content/drive\" + \"/MyDrive\" + \"/Cursos/Coderhouse/Data Science/\"\n",
    "path = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c30e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype\n",
    "from scipy.stats import ttest_ind\n",
    "import sidetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51126a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_info(new_df):\n",
    "  df_info = pd.DataFrame(columns=['Count', 'Type' , 'Missing', 'Unique', 'Numeric'])\n",
    "  for col in new_df:\n",
    "    data_series = new_df[col]\n",
    "    df_info.loc[col] = [data_series.count(), data_series.dtype, data_series.isnull().sum(), data_series.nunique(), is_numeric_dtype(data_series)]\n",
    "    df_describe = new_df.describe(include='all').T[['top', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']]\n",
    "    df_stats = pd.DataFrame([new_df.skew(numeric_only=True), new_df.kurtosis(numeric_only=True)], index=['skew', 'kurtosis']).T\n",
    "  return pd.concat([df_info,pd.concat([df_describe, df_stats], axis=1)], axis=1).fillna('0')\n",
    "\n",
    "def corrFilter(x: pd.DataFrame, thres: float):\n",
    "    #generate corr \n",
    "    xCorr = x.corr('spearman')\n",
    "    #filter corr by thres\n",
    "    xFiltered = xCorr[((xCorr >= thres) | (xCorr <= -thres)) & (xCorr !=1.000)]\n",
    "    #change dataframe format\n",
    "    xFlattened = xFiltered.unstack().drop_duplicates().reset_index().sort_values(0, ascending= False).dropna()\n",
    "    #rename columns\n",
    "    xFlattened.columns = ['Variable_1', 'Variable_2', 'corr_value']\n",
    "    return xFlattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00096e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strat_sample(df, target, sample_size, seed): \n",
    "    '''We make a stratified sample by a binary target. The target values are 0 and 1, \n",
    "                                                #for simplicity.\n",
    "                                                \n",
    "        df is a dataframe\n",
    "        \n",
    "        target is the name of the target variable\n",
    "        \n",
    "        seed is the random seed to generate pseudo random numbers.'''\n",
    "    \n",
    "    x=df[target].value_counts(normalize=True)*sample_size # Calculate the rate of each target value and then we multiply by\n",
    "        # sample size. If we around this numbers we obtain the desired number of samples for each posible value of the target\n",
    "        # variable. These only works because the target is binary. If the target is not binary, I have to think of a way to\n",
    "        # round all the rows and have the sum of exactly the number of samples desired\n",
    "   \n",
    "    x=round(x,0)\n",
    "    size_0 =x.iloc[0]\n",
    "    size_0=size_0.astype(int)\n",
    "    size_1 = x.loc[1]\n",
    "    size_1=size_1.astype(int)\n",
    "    \n",
    "    part_0 = df[df[target]==0].sample(size_0, random_state= seed) # We take a sample for each value of the sample of size_i.\n",
    "    part_1 = df[df[target]==1].sample(size_1, random_state= seed)\n",
    "    sample = pd.concat([part_0, part_1])\n",
    "    \n",
    "    return sample    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63302269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_values(df, category_col, column_to_analyze):\n",
    "  '''It does the same as df.groupby(category_col)[column_to_analyze].describe() but with more information. But it has \n",
    "  the problem that it couldn't handle null values. We have to see how to fix it later.'''\n",
    "\n",
    "  #create a dataframe with specific columns\n",
    "\n",
    "  df_info = pd.DataFrame(columns=['count', 'missing', 'unique_values', 'mean', 'std', 'mode', 'min', '25%', '50%', '75%', 'max', 'skew', 'kurtosis'])\n",
    "  df_info.index.name = category_col # The name in the index appears in the name of the first column\n",
    "  #loop of all the values that the category has\n",
    "  for val in df[category_col].unique():\n",
    "\n",
    "      # get info from column\n",
    "      data_series = df[df[category_col]==val][column_to_analyze]\n",
    "      # fill dataframe with initial columns\n",
    "      df_info.loc[val] = [data_series.count(), data_series.isnull().sum(),  data_series.nunique(), data_series.mean(), data_series.std(), data_series.mode().iloc[0], data_series.min(), data_series.quantile(.25), data_series.quantile(.5), data_series.quantile(.75), data_series.max(), data_series.skew(), data_series.kurtosis()]\n",
    "\n",
    "  return df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c0cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_log(a,b,n):\n",
    "    '''Given an integer interval [a,b](={a, a+1, ..., b}) returns a partition {a=a_0, a_1, a_2, ..., a_n=b} \"logarithmically \n",
    "    equispaced in integer numbers\". By this we mean that ln (a_{i+1}-a_i +1) is constant, or equivalently,\n",
    "    (a_{i+1}-a_0+1)/((a_{i}-a_0+1)) is constant. (To understand why 1 is added, think about the interval [0,1] and n=1. You have\n",
    "    #to transform the interval to [1,2]).'''\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    s=np.log(b-a+1)/n\n",
    "    l=np.array([])\n",
    "    for j in range(0,n+1):\n",
    "        l=np.append(l, np.exp(j*s)-1+a)\n",
    "    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edea4d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlxtend \n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dce1264",
   "metadata": {},
   "outputs": [],
   "source": [
    "styles=[dict(selector=\"caption\", props=[(\"font-size\", \"100%\"), (\"font-weight\", \"bold\")\n",
    "             #            ,(\"color\", \"white\"), (\"background-color\", \"grey\")])] # Another option                                       \n",
    "                                       ])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.compose import ColumnTransformer # Code to use feature cat and numeric together\n",
    "from sklearn.compose import make_column_selector # Selector in pipeline by dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9abe235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score as bas\n",
    "import time\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f4ded7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da20ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1555bab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c86256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803be9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43d20e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e753ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
